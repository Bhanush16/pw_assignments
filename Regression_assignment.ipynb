{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**1- What is Simple Linear Regression?**\n",
        "\n",
        "A) Simple Linear Regression is a statistical method used to model the relationship between a single independent variable (X) and a dependent variable (Y) by fitting a straight line in the form **Y = mX + c**. It helps in predicting the value of Y based on X.\n",
        "\n",
        "**2- What are the key assumptions of Simple Linear Regression?**\n",
        "\n",
        "A)\n",
        "\n",
        "* Linearity: Relationship between X and Y is linear.\n",
        "* Independence: Observations are independent.\n",
        "* Homoscedasticity: Constant variance of residuals.\n",
        "* Normality: Residuals are normally distributed.\n",
        "* No multicollinearity (trivial here since only one predictor).\n",
        "\n",
        "**3- What does the coefficient m represent in the equation Y = mX + c?**\n",
        "\n",
        "A) The coefficient **m** represents the **slope** of the regression line, i.e., the change in Y for a one-unit change in X.\n",
        "\n",
        "**4- What does the intercept c represent in the equation Y = mX + c?**\n",
        "\n",
        "A) The intercept **c** is the value of Y when X = 0. It represents the starting point of the regression line.\n",
        "\n",
        "**5- How do we calculate the slope m in Simple Linear Regression?**\n",
        "\n",
        "A) The slope is calculated as:\n",
        "\n",
        "$$\n",
        "m = \\frac{\\sum (X_i - \\bar{X})(Y_i - \\bar{Y})}{\\sum (X_i - \\bar{X})^2}\n",
        "$$\n",
        "\n",
        "**6- What is the purpose of the least squares method in Simple Linear Regression?**\n",
        "\n",
        "A) The least squares method minimizes the sum of squared residuals (errors) between observed and predicted values, ensuring the best-fit line.\n",
        "\n",
        "**7- How is the coefficient of determination (R²) interpreted in Simple Linear Regression?**\n",
        "\n",
        "A) R² measures the proportion of variance in the dependent variable explained by the independent variable. A higher R² means better model fit.\n",
        "\n",
        "**8- What is Multiple Linear Regression?**\n",
        "\n",
        "A) Multiple Linear Regression models the relationship between a dependent variable (Y) and two or more independent variables (X₁, X₂, …, Xn).\n",
        "\n",
        "**9- What is the main difference between Simple and Multiple Linear Regression?**\n",
        "\n",
        "A) Simple Linear Regression has only one independent variable, while Multiple Linear Regression has two or more.\n",
        "\n",
        "**10- What are the key assumptions of Multiple Linear Regression?**\n",
        "\n",
        "A)\n",
        "\n",
        "* Linearity between predictors and outcome.\n",
        "* Independence of observations.\n",
        "* Homoscedasticity of residuals.\n",
        "* Normal distribution of residuals.\n",
        "* No or low multicollinearity.\n",
        "\n",
        "**11- What is heteroscedasticity, and how does it affect the results of a Multiple Linear Regression model?**\n",
        "\n",
        "A) Heteroscedasticity occurs when residuals have non-constant variance. It leads to inefficient estimates and invalid standard errors, affecting hypothesis testing.\n",
        "\n",
        "**12- How can you improve a Multiple Linear Regression model with high multicollinearity?**\n",
        "\n",
        "A)\n",
        "\n",
        "* Remove highly correlated variables.\n",
        "* Use dimensionality reduction (e.g., PCA).\n",
        "* Apply regularization (Ridge or Lasso regression).\n",
        "\n",
        "**13- What are some common techniques for transforming categorical variables for use in regression models?**\n",
        "\n",
        "A)\n",
        "\n",
        "* One-hot encoding.\n",
        "* Label encoding.\n",
        "* Dummy variables.\n",
        "\n",
        "**14- What is the role of interaction terms in Multiple Linear Regression?**\n",
        "\n",
        "A) Interaction terms capture the combined effect of two variables on the outcome when their joint effect differs from the sum of individual effects.\n",
        "\n",
        "**15- How can the interpretation of intercept differ between Simple and Multiple Linear Regression?**\n",
        "\n",
        "A) In Simple Linear Regression, the intercept represents Y when X=0. In Multiple Linear Regression, it represents Y when all independent variables are zero, which may or may not be meaningful.\n",
        "\n",
        "**16- What is the significance of the slope in regression analysis, and how does it affect predictions?**\n",
        "\n",
        "A) The slope indicates how much the dependent variable changes with a one-unit increase in the independent variable, holding others constant. It directly affects predictions.\n",
        "\n",
        "**17- How does the intercept in a regression model provide context for the relationship between variables?**\n",
        "\n",
        "A) The intercept provides the baseline value of the dependent variable when predictors are zero, serving as a reference point for interpretation.\n",
        "\n",
        "**18- What are the limitations of using R² as a sole measure of model performance?**\n",
        "\n",
        "A)\n",
        "\n",
        "* R² always increases with more variables, even if they add no predictive power.\n",
        "* It does not indicate overfitting.\n",
        "* It does not confirm causation.\n",
        "\n",
        "**19- How would you interpret a large standard error for a regression coefficient?**\n",
        "\n",
        "A) A large standard error suggests the coefficient estimate is unstable and not significantly different from zero, lowering confidence in the predictor’s effect.\n",
        "\n",
        "**20- How can heteroscedasticity be identified in residual plots, and why is it important to address it?**\n",
        "\n",
        "A) Heteroscedasticity is identified when residual plots show a funnel or pattern rather than random scatter. Addressing it is important to ensure valid statistical inference.\n",
        "\n",
        "**21- What does it mean if a Multiple Linear Regression model has a high R² but low adjusted R²?**\n",
        "\n",
        "A) It means that additional predictors are not contributing useful information and may just be overfitting the model.\n",
        "\n",
        "**22- Why is it important to scale variables in Multiple Linear Regression?**\n",
        "\n",
        "A) Scaling helps when predictors have different units or magnitudes, ensuring coefficients are comparable and avoiding numerical instability.\n",
        "\n",
        "**23- What is polynomial regression?**\n",
        "\n",
        "A) Polynomial regression is a type of regression where the relationship between X and Y is modeled as an nth-degree polynomial.\n",
        "\n",
        "**24- How does polynomial regression differ from linear regression?**\n",
        "\n",
        "A) Linear regression models a straight-line relationship, while polynomial regression models curved relationships by adding higher-order terms.\n",
        "\n",
        "**25- When is polynomial regression used?**\n",
        "\n",
        "A) It is used when data shows a nonlinear relationship between X and Y that cannot be captured by a straight line.\n",
        "\n",
        "**26- What is the general equation for polynomial regression?**\n",
        "\n",
        "A)\n",
        "\n",
        "$$\n",
        "Y = b_0 + b_1X + b_2X^2 + b_3X^3 + \\dots + b_nX^n + \\epsilon\n",
        "$$\n",
        "\n",
        "**27- Can polynomial regression be applied to multiple variables?**\n",
        "\n",
        "A) Yes, polynomial regression can be extended to multiple predictors with polynomial combinations of variables.\n",
        "\n",
        "**28- What are the limitations of polynomial regression?**\n",
        "\n",
        "A)\n",
        "\n",
        "* Overfitting with high-degree polynomials.\n",
        "* Poor extrapolation beyond observed data.\n",
        "* Increased computational complexity.\n",
        "\n",
        "**29- What methods can be used to evaluate model fit when selecting the degree of a polynomial?**\n",
        "\n",
        "A)\n",
        "\n",
        "* Cross-validation.\n",
        "* Adjusted R².\n",
        "* AIC/BIC criteria.\n",
        "* Residual analysis.\n",
        "\n",
        "**30- Why is visualization important in polynomial regression?**\n",
        "\n",
        "A) Visualization helps identify nonlinear trends, assess curve fitting, and avoid overfitting by visually inspecting how well the polynomial fits the data.\n",
        "\n",
        "**31- How is polynomial regression implemented in Python?**\n",
        "\n",
        "A) Polynomial regression can be implemented using:\n",
        "\n",
        "```python\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "model = make_pipeline(PolynomialFeatures(degree=3), LinearRegression())\n",
        "model.fit(X, y)\n",
        "y_pred = model.predict(X)\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "HNd69Wk7WhWC"
      }
    }
  ]
}